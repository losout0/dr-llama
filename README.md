# Dr. Llama ğŸ¦™âš–ï¸

Seu assistente de IA para informaÃ§Ãµes sobre a legislaÃ§Ã£o brasileira.

**Dr. Llama** Ã© uma Prova de Conceito (PoC) de um assistente jurÃ­dico informativo, construÃ­do com uma arquitetura de **RAG (Retrieval-Augmented Generation)** e **Agentes de IA**. O sistema foi desenvolvido como projeto final para a disciplina de LLMs e tem como objetivo democratizar o acesso a informaÃ§Ãµes sobre as leis brasileiras de forma clara e referenciada.

âš ï¸ **Disclaimer:** Dr. Llama Ã© uma ferramenta experimental para fins informativos. NÃ£o Ã© um substituto para aconselhamento jurÃ­dico profissional.

## ğŸ“œ Ãndice

- [ğŸ¯Problema e Objetivo](#-problema-e-objetivo)
- [âœ¨Funcionalidades](#-funcionalidades)
- [ğŸ—ï¸Arquitetura](#-arquitetura)
- [ğŸš€Como Executar Localmente](#-como-executar-localmente)
  - [PrÃ©-requisitos](#-prÃ©-requisitos)
  - [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
  - [Executando com Docker](#-executando-com-docker)
- [ğŸ“‚Estrutura do RepositÃ³rio](#-estrutura-do-repositÃ³rio)
- [ğŸ“ŠAvaliaÃ§Ã£o](#-avaliaÃ§Ã£o)
- [âš–ï¸LimitaÃ§Ãµes Ã‰ticas e de SeguranÃ§a](#-limitaÃ§Ãµes-Ã©ticas-e-de-seguranÃ§a)
- [ğŸ—ºï¸Roadmap (PrÃ³ximos Passos)](#-roadmap-prÃ³ximos-passos)
- [ğŸ“„LicenÃ§a](#-licenÃ§a)

## ğŸ¯ Problema e Objetivo

O acesso Ã  informaÃ§Ã£o jurÃ­dica no Brasil Ã© um desafio para o cidadÃ£o comum. A linguagem tÃ©cnica e a estrutura complexa das leis dificultam a compreensÃ£o de direitos e deveres bÃ¡sicos.

O objetivo do **Dr. Llama** Ã© mitigar esse problema, oferecendo uma interface conversacional que responde a perguntas sobre a legislaÃ§Ã£o brasileira com base em fontes oficiais. O sistema utiliza tÃ©cnicas de RAG para evitar alucinaÃ§Ãµes e garantir que todas as respostas sejam fundamentadas e citem os artigos de lei correspondentes.

## âœ¨ Funcionalidades

- ğŸ’¬ **Interface Conversacional:** Dialogue com o sistema em linguagem natural.
- ğŸ“š **Respostas Baseadas em EvidÃªncias:** As respostas sÃ£o geradas a partir de um corpus de documentos legais oficiais (ConstituiÃ§Ã£o Federal, CÃ³digo de Defesa do Consumidor, etc.).
- ğŸ”— **CitaÃ§Ãµes de Fontes:** Cada resposta inclui referÃªncias explÃ­citas aos artigos de lei utilizados, permitindo a verificaÃ§Ã£o da informaÃ§Ã£o.
- ğŸ¤– **OrquestraÃ§Ã£o com Agentes (LangGraph):** Um grafo de agentes gerencia o fluxo da conversa, desde a recuperaÃ§Ã£o da informaÃ§Ã£o atÃ© a checagem de seguranÃ§a e formataÃ§Ã£o da resposta.
- âœ… **Checagem Anti-AlucinaÃ§Ã£o:** Um agente _SelfCheck_ valida se as informaÃ§Ãµes na resposta estÃ£o de fato presentes nos documentos recuperados.
- âš™ï¸ **100% Open-Source e Local:** Utiliza modelos de LLM open-weights (via Ollama) e bancos de vetores locais (FAISS), garantindo privacidade e total controle sobre o sistema.

## ğŸ—ï¸ Arquitetura

O Dr. Llama Ã© orquestrado pelo **LangGraph**, que coordena uma equipe de agentes especializados. O fluxo de uma pergunta Ã© o seguinte:

```mermaid
graph TD
    A[UsuÃ¡rio via UI Streamlit] --> B{LangGraph Supervisor};
    B --> C[1. RetrieverAgent];
    C --> D[VectorStore FAISS];
    C --> B;
    B --> E[2. AnswerAgent];
    E --> F[LLM via Ollama];
    E --> B;
    B --> G[3. SelfCheckAgent];
    G -- EvidÃªncia OK --> H[4. SafetyAgent];
    G -- EvidÃªncia Insuficiente --> E;
    H --> I[Resposta Final com CitaÃ§Ãµes e Disclaimer];
    I --> A;
```

- **UI (Streamlit):** Interface web onde o usuÃ¡rio interage com o sistema.
- **LangGraph Supervisor:** O "maestro" que roteia a tarefa entre os diferentes agentes com base no estado atual da conversa.
- **RetrieverAgent:** ResponsÃ¡vel por buscar os trechos de lei mais relevantes para a pergunta do usuÃ¡rio no banco de vetores FAISS.
- **AnswerAgent:** Gera uma resposta em linguagem natural, utilizando o contexto fornecido pelo RetrieverAgent e citando as fontes.
- **SelfCheckAgent:** Compara a resposta gerada com os documentos originais para garantir a fidelidade e evitar a invenÃ§Ã£o de informaÃ§Ãµes.
- **SafetyAgent:** Adiciona o disclaimer legal a todas as respostas, reforÃ§ando o carÃ¡ter informativo da ferramenta.

**Stack TecnolÃ³gica:** Python, LangChain, LangGraph, Ollama (Llama 3.1 8B), FAISS, HuggingFace Embeddings (gte-small), Streamlit, Docker.

## ğŸš€ Como Executar Localmente

### PrÃ©-requisitos

- Git
- Python 3.10+
- Docker
- Ollama

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**

```bash
git clone https://github.com/SEU-USUARIO/dr-llama.git
cd dr-llama
```

2. **Configure o Ollama e baixe o LLM:**

- Siga as instruÃ§Ãµes para instalar o Ollama no seu sistema.
- Baixe o modelo Llama 3.1:

```Bash
ollama pull llama3.1:8b
```

3. **Crie um ambiente virtual e instale as dependÃªncias:**

```Bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

4. **Prepare os dados e o banco de vetores:**

- Adicione os arquivos de lei (ex: constituicao.pdf, cdc.pdf) na pasta /data/raw.
- Execute o script de ingestÃ£o para criar o Ã­ndice FAISS:

```Bash
python ingest/ingest_data.py
```

5. **Inicie a aplicaÃ§Ã£o:**

```Bash
streamlit run app/app.py
```

Abra seu navegador em `http://localhost:8501`.

**Executando com Docker**
ApÃ³s clonar o repositÃ³rio e rodar o script de ingestÃ£o (passos 1 e 4), vocÃª pode construir e executar o container Docker:

```Bash
# Construa a imagem
docker build -t dr-llama .
```

```bash
# Execute o container
docker run -p 8501:8501 dr-llama
```

### ğŸ“‚ Estrutura do RepositÃ³rio

```bash
/dr-llama
|
â”œâ”€â”€ .github/workflows/      # Pipelines de CI/CD
â”œâ”€â”€ app/                    # CÃ³digo da interface Streamlit
â”œâ”€â”€ data/                   # Dados brutos (PDFs) e processados
â”œâ”€â”€ ingest/                 # Scripts para processamento e indexaÃ§Ã£o dos dados
â”œâ”€â”€ src/                    # LÃ³gica principal: agentes, grafo, etc.
â”œâ”€â”€ eval/                   # Scripts e relatÃ³rios de avaliaÃ§Ã£o (RAGAS)
â”œâ”€â”€ tests/                  # Testes unitÃ¡rios e de integraÃ§Ã£o
|
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile              # ContainerizaÃ§Ã£o da aplicaÃ§Ã£o
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt        # DependÃªncias Python
```

### ğŸ“Š AvaliaÃ§Ã£o

A qualidade do sistema Ã© medida utilizando o framework **RAGAS**. Nosso processo de avaliaÃ§Ã£o inclui:

- Um conjunto de **20-30 perguntas** de teste com respostas de referÃªncia, localizadas em `eval/test_questions.json`.
- MÃ©tricas principais: `Faithfulness`, `Answer Relevancy`, `Context Precision` e `Context Recall`.
- Os resultados detalhados e a anÃ¡lise crÃ­tica da performance estÃ£o disponÃ­veis no relatÃ³rio `eval/report.md`.

### âš–ï¸ LimitaÃ§Ãµes Ã‰ticas e de SeguranÃ§a

- **NÃƒO Ã© Aconselhamento JurÃ­dico:** Dr. Llama Ã© uma ferramenta de informaÃ§Ã£o, nÃ£o um consultor legal. As respostas nÃ£o criam uma relaÃ§Ã£o advogado-cliente.
- **InformaÃ§Ã£o Potencialmente Desatualizada**: O corpus de conhecimento Ã© estÃ¡tico e baseado nos documentos fornecidos na data da ingestÃ£o. Leis podem ser alteradas.
- **Sem Garantia de PrecisÃ£o**: Embora utilize RAG para mitigar alucinaÃ§Ãµes, erros de interpretaÃ§Ã£o ou recuperaÃ§Ã£o podem ocorrer. Sempre verifique as fontes citadas.
- **Complexidade do Caso**: O sistema nÃ£o considera as nuances e particularidades de um caso real, que sÃ£o essenciais para uma orientaÃ§Ã£o jurÃ­dica adequada.

### ğŸ—ºï¸ Roadmap (PrÃ³ximos Passos)

- [ ] **Expandir o Corpus:** Incluir mais documentos legais (CLT, CÃ³digo Civil, etc.).
- [ ] **Melhorar o Retrieval:** Implementar tÃ©cnicas de re-ranking (Cross-Encoders) para melhorar a relevÃ¢ncia dos documentos.
- [ ] **AvaliaÃ§Ã£o ContÃ­nua:** Criar um workflow de CI/CD que rode a suÃ­te de avaliaÃ§Ã£o a cada mudanÃ§a no cÃ³digo.
- [ ] **Deploy:** Publicar a aplicaÃ§Ã£o em uma plataforma como Hugging Face Spaces ou Streamlit Community Cloud.

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a APACHE 2.0. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
